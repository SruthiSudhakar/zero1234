{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05297ef3",
   "metadata": {},
   "source": [
    "# dataloading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf941a0",
   "metadata": {},
   "source": [
    "# RTMV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "22fbec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import imageio\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "261fc372",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/rliu/Desktop/cvfiler04/datasets/RTMV/google_scanned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ace98041",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = next(os.walk(root_dir))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "10e22061",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = '/home/rliu/Desktop/cvfiler04/datasets/RTMV/google_scanned/00000/transforms.json'\n",
    "with open(json_path, \"r\") as f:\n",
    "    meta = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7b8c86d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.36230278e-01, -2.53794730e-01,  4.86114502e-01,\n",
       "         9.35986042e-01],\n",
       "       [ 5.48378587e-01, -3.87015104e-01,  7.41282940e-01,\n",
       "         1.43672696e+00],\n",
       "       [-1.49011612e-08,  8.86457920e-01,  4.62809265e-01,\n",
       "         1.02960816e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(meta['frames'][0]['transform_matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "58673e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RTMV(Dataset):\n",
    "    def __init__(self, root_dir='/home/rliu/Desktop/cvfiler04/datasets/RTMV/google_scanned',\\\n",
    "                 first_K=64, resolution=256, load_target=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.scene_list = next(os.walk(root_dir))[1]\n",
    "        self.resolution = resolution\n",
    "        self.first_K = first_K\n",
    "        self.load_target = load_target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.scene_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scene_dir = os.path.join(self.root_dir, self.scene_list[idx])\n",
    "        with open(os.path.join(scene_dir, 'transforms.json'), \"r\") as f:\n",
    "            meta = json.load(f)\n",
    "        imgs = []\n",
    "        poses = []\n",
    "        for i_img in range(self.first_K):\n",
    "            meta_img = meta['frames'][i_img]\n",
    "\n",
    "            if i_img == 0 or self.load_target:\n",
    "                img_path = os.path.join(scene_dir, meta_img['file_path'])\n",
    "                img = imageio.imread(img_path)\n",
    "                img = cv2.resize(img, (self.resolution, self.resolution), interpolation = cv2.INTER_LINEAR)\n",
    "                imgs.append(img)\n",
    "            \n",
    "            c2w = meta_img['transform_matrix']\n",
    "            poses.append(c2w)\n",
    "            \n",
    "        imgs = (np.array(imgs) / 255.).astype(np.float32)  # (RGBA) imgs\n",
    "        imgs = torch.tensor(self.blend_rgba(imgs))\n",
    "        imgs = (imgs + 1.) / 2. # convert to stable diffusion range\n",
    "        poses = torch.tensor(np.array(poses).astype(np.float32))\n",
    "        return imgs, poses\n",
    "                \n",
    "    def blend_rgba(self, img):\n",
    "        img = img[..., :3] * img[..., -1:] + (1. - img[..., -1:])  # blend A to RGB\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "087d2087",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RTMV(load_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "54e672a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img, poses = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7497108a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 4, 4])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b21d1bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_im, poses = dataset[0]\n",
    "\n",
    "input_im = input_im[0]\n",
    "input_pose = poses[0]\n",
    "target_poses = poses[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "acb74426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.6184e+00,  7.5305e-01, -2.2068e+00, -5.0729e-01],\n",
       "        [-2.3318e+00,  2.1374e+00, -6.2638e+00, -1.4399e+00],\n",
       "        [-5.2282e-08,  6.6411e+00,  2.2662e+00,  5.2094e-01],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_poses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8da8cf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_to_spherical(xyz):\n",
    "    ptsnew = np.hstack((xyz, np.zeros(xyz.shape)))\n",
    "    xy = xyz[:,0]**2 + xyz[:,1]**2\n",
    "    z = np.sqrt(xy + xyz[:,2]**2)\n",
    "    theta = np.arctan2(np.sqrt(xy), xyz[:,2]) # for elevation angle defined from Z-axis down\n",
    "    #ptsnew[:,4] = np.arctan2(xyz[:,2], np.sqrt(xy)) # for elevation angle defined from XY-plane up\n",
    "    azimuth = np.arctan2(xyz[:,1], xyz[:,0])\n",
    "    return np.array([theta, azimuth, z])\n",
    "\n",
    "\n",
    "def get_T(T_target, T_cond):\n",
    "    theta_cond, azimuth_cond, z_cond = cartesian_to_spherical(T_cond[None, :])\n",
    "    theta_target, azimuth_target, z_target = cartesian_to_spherical(T_target[None, :])\n",
    "    \n",
    "    d_theta = theta_target - theta_cond\n",
    "    d_azimuth = (azimuth_target - azimuth_cond) % (2 * math.pi)\n",
    "    d_z = z_target - z_cond\n",
    "    \n",
    "    d_T = torch.tensor([d_theta.item(), math.sin(d_azimuth.item()), math.cos(d_azimuth.item()), d_z.item()])\n",
    "    return d_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "69e0d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_T = []\n",
    "for i_frame in range(len(target_poses)):\n",
    "    T = get_T(target_poses[i_frame][:3, -1].numpy(), input_pose[:3, -1].numpy())[None, :]\n",
    "    x_T.append(T)\n",
    "x_T = torch.cat(x_T)[:, None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c2f3847a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5bf813db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 4])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e1ec80",
   "metadata": {},
   "source": [
    "# GSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4d7193c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSO(Dataset):\n",
    "    def __init__(self, root_dir='/home/rliu/Desktop/cvfiler04/datasets/GoogleScannedObjects',\\\n",
    "                 split='val', first_K=5, resolution=256, load_target=False):\n",
    "        self.root_dir = root_dir\n",
    "        with open(os.path.join(root_dir, '%s.json' % split), \"r\") as f:\n",
    "            self.scene_list = json.load(f)\n",
    "        self.resolution = resolution\n",
    "        self.first_K = first_K\n",
    "        self.load_target = load_target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.scene_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scene_dir = os.path.join(self.root_dir, self.scene_list[idx])\n",
    "        with open(os.path.join(scene_dir, 'transforms_render_mvs.json'), \"r\") as f:\n",
    "            meta = json.load(f)\n",
    "        imgs = []\n",
    "        poses = []\n",
    "        for i_img in range(self.first_K):\n",
    "            meta_img = meta['frames'][i_img]\n",
    "            \n",
    "            if i_img == 0 or self.load_target:\n",
    "                img_path = os.path.join(scene_dir, meta_img['file_path'])\n",
    "                img = imageio.imread(img_path)\n",
    "                img = cv2.resize(img, (self.resolution, self.resolution), interpolation = cv2.INTER_LINEAR)\n",
    "                imgs.append(img)\n",
    "            \n",
    "            c2w = meta_img['transform_matrix']\n",
    "            poses.append(c2w)\n",
    "            \n",
    "        imgs = (np.array(imgs) / 255.).astype(np.float32)  # (RGBA) imgs\n",
    "        mask = imgs[:, :, :, -1]\n",
    "        imgs = torch.tensor(self.blend_rgba(imgs))\n",
    "        imgs = (imgs + 1.) / 2. # convert to stable diffusion range\n",
    "        poses = torch.tensor(np.array(poses).astype(np.float32))\n",
    "        return imgs, poses\n",
    "                \n",
    "    def blend_rgba(self, img):\n",
    "        img = img[..., :3] * img[..., -1:] + (1. - img[..., -1:])  # blend A to RGB\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "eae745a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GSO('/home/rliu/Desktop/cvfiler04/datasets/GoogleScannedObjects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fbda51a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, poses = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a79a9eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 256, 3])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b4637cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 4])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poses.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
